%!TEX root = ../main.tex
\documentclass[a4paper,oneside,12pt, class=Latex/Classes/PhDthesisPSnPDF, crop=false]{standalone}
\usepackage{setspace}
\begin{document}
\doublespacing
\chapter{Observing in the optical regime}
\label{chap:obs}

Astrophysicists face the unusual challenge of not being able to control their experiments. The universe is our laboratory but all we can do is observe the results while often not knowing the exact setup of the experiment. Models are made to explain and predict the behaviour of planets, stars, galaxies, etc. but ultimately observations are needed to compare against and test our models. My work relies heavily on observational data, and in this chapter I will introduce the telescopes and instruments that are at the basis of this thesis. I will also give a general overview of what to consider when planning observations and different types of observations that can be done in the optical regime. \color{red}add refs to sections, reduction and analysis in a separate chapter probably \color{black}


\section{Telescopes}
Most of the data used in this thesis comes from the Zwicky Transient Facility (ZTF), and follow-up observations have been made using the Nordic Optical Telescope (NOT), and the Gran Telescopio Canarias (GTC), which will be introduced below. Some additional data comes from other sources, which we list for completeness. \color{red}ow subsection or list here \color{black}

\subsection{Zwicky Transient Facility}
The Zwicky Transient Facility (ZTF) is an optical large-sky survey observing the entire northern night sky above Dec $\sim-30$\degree every 2-3 nights in three broadband optical filters \ztfg\ztfr\ztfi, which are very similar to the well-known SDSS \ztfg\ztfr\ztfi\ filters. The efficiency of these filters is plotted as a function of wavelength in Fig.\ref{Optical_elements_plot}. The survey saw first light in October 2017 and the survey formally began scientific operation in March 2018 and has been running continueously until the time of writing this document.

The observations are made using the $48\arcsec$ aperture Schmidt-type design Samuel Oschin Telescope, which is based at the Palomar Observatory in Southern California. Each exposure, lasting 30 s, can go a limiting magnitude of $\sim20.5$ mag and covers an area of $\sim47$ deg$^2$ at a resolution of of $1.01\arcsec$ per pixel. The camera is divided in a $4\times4$ grid of CCDs, each of which have 4 readout channels called quadrants. This result in each observation producing 64 separate images, each with their own quadrant identifier (qid). Similarly, the observed region of the sky is divided into different telescope pointings called fields (identified using fid) to ensure that the same region of the sky is observed in the same way each time, aiding with the reduction of the data. This results in each combination of filter, fid, and qid being a set of observations of a particular part of the sky using speciic setup. \color{red} Add all the usual ZTF references \color{black}

\begin{figure}
    \centering
    %\includegraphics[width=\textwidth]{}
    \caption{Throughput as a function of wavelength of the different filters used to gather the bulk of the data in this thesis. The wavelength ranges of the elemtents used for spectroscopy are shown as well. \color{red} different colours for filters, different linestyles for filter sets, add legend, update caption \color{black}}
    \label{Optical_elements_plot}
\end{figure}

\subsection{Nordic Optical Telescope}
The Nordic Optical Telescope (NOT) is a 2.56 m telescope located at Roque the Los Muchacos in La Palma, Spain. \color{red} Add elevation and coords? \color{black} It hosts several instruments for observing in the optical and near infrared, both for imaging and spectroscopy. The main instrument is the Alhambra Faint Object Spectrograph and Camera (ALFOSC), which was used to obtain the data used in this thesis. I will only discuss the parts relevant to this thesis, further details on this instrument and  details on the other instruments can be found at \footnote{\url{https://not.iac.es}}.

ALFOSC is a versatile instrument that can be used for imaging, spectroscopy, and (spectro)polarimtery. As there are several wheels equipped to hold a variety of optical elements, the instruments can switch quickly between different setups between observations. The images can cover up to $6.4\times6.4\arcmin$ per exposure at a resolution of $0.2138\arcsec$ per pixel. For photometry filters 120 (\ztfg$\arcmin$), 110 (\ztfr$\arcmin$), and 111 (\ztfi$\arcmin$) are used. For spectroscopy grism 4 is used together with a $1.0\arcsec$ slit if the seeing was $\leq1.3\arcsec$ or a  $1.3\arcsec$ slit if the seeing was $\geq1.3\arcsec$. For some spectra an order-blocking filter (WG345) is used as well to avoid second order diffracted blue light to overlap with first order diffracted red light on the detector. Details on these optical elements are given in \ref{NOT_optic_elems}, and they are plotted as a funtion of wavelength in \ref{Optical_elements_plot}.

\begin{table}
    \centering
    \caption{Optical elements used for observations taken with NOT/ALFOSC. \color{red}Finish, add GTC in here as well instead of its own? Don't really like the format \color{black}}
    %\resizebox{\textwidth}{!}{%Scale table to page width
    	\begin{tabular}{ccccc}
    		\hline
    		\hline
    		Filter & $\lambda_\text{center}$ (\AA) & FWHM (\AA) & T$_\text{max}$\\
    		\hline
    		\ztfg$\arcmin$ SDSS & 4800 & 1450 & 0.92\\
    		\ztfr$\arcmin$ SDSS & 6180 & 1480 & 0.90\\
    		\ztfi$\arcmin$ SDSS & 7710 & 1710 & 0.91\\
    		WG345 & 3560 & - & 0.88\\
    		\\
    		\hline
    		\hline
    		Grism & $\lambda$ range (\AA) & resolution (\AA / pixel) & Orientation\\
    		\hline
    		\#4 & 3200 - 9600 & 3.3 & vertical\\
    		\\
    		\hline
    		\hline
    		slit & & Orientation\\
    		\hline
    		1.0$\arcsec$ && horizontal & \\
    		1.3$\arcsec$ && horizontal & \\
    		\hline
    	\end{tabular}
    %}
    \label{NOT_optic_elems}
\end{table}

\subsection{Gran Telescopio Canarias}


\subsection{Other observations}


\section{General considerations}
To observe astronomical objects one has to consider several things. Assuming that the location or region on the sky that we are interested in to observe is already known, an observing plan can be made. A well constructed observing plan should give the best quality data possible while making efficient use of the resources available. Several telescopes were used to obtain the data used in this thesis. The main ones are the \color{red}ZTF one, find a good place to properly introduce ZTF,the telescopes, and the main instruments \color{black}, the Gran Telescopio de Canarias (GTC), and the Nordic Optical Telescope (NOT). Below I will use the NOT as a specific example.


\subsection{Location}
For a complete picture, the first thing to consider is the location from where to observe. Usually there is a limit to the freedom of choice, given that most astronomers will use telescopes that are already built somewhere. This consideration is of more importance when deciding on a place for a new telescope. I will disregard the socioenonomic considerations for such a project and assume, for simplicity, that I can put my telescope wherever is best.

There are three main things to consider when chosing a location:
\begin{itemize}
	\item {Weather: There is no point in trying to observe when it is cloudy all the time. Locations with many clear nights and stable weather are prefered. On top of that, by going to higher elevation, lower hanging clouds can be avoided, while simultaneously decreasing the amount of air starlight has to travel through to reach the detector. All of this can ultimately be avoided by putting the telescope into outer space, although that comes with its own set of challenges.}
	\item {Light pollution: The darker the sky, the fainter objects can be observed. With the abundant usage of lights in modern day society, the night sky has become polluted with artificial light sources to the point that it has become difficult to see even the brightest stars in large urbanized areas. Moving away from such areas, and to places where the ligth pollution is controlled by law is imperative to obtain good quality data.}
	\item {Target observability: The closer to zenith an observation is made, the better quality data as it decreases the amount of atmosphere between the telescope and light source. If you want to focus on northern targets observe from the northern hemisphere, if you want to focus on southern targets observe from the southern hemisphere, if you want to get a bit of both, build near the equator. The atmosphere can reduce the data quality through turbulence (seeing), broadband absorbtion (clouds, dust), and narrowband interference (tellurics, skylines), and achromatic diffraction (different colours diffracting differently when entering the atmosphere, \color{red} Check name and cite that specific paper \color{black}) among others.}
\end{itemize}

These considerations are also affected by the time of the year. Winter has longer nights than summer, but the colder weather can lead to more chance of precipitation or water freezing to the dome, preventing the safe use of the telescope. Also, as the earth moves around the sun different parts of the night sky become observable at different times throughout the night, in many cases making it impossible to observe a target at any time throughout the year.

The NOT is located on the island of La Palma, Spain at Roque the Los Muchachos, m above sealevel, at a latitude of \color{red}x\color{black}\degree, and as the telescope can go down to 7\degree above the horizon this means that theoretically the entire northern sky down to Dec \color{red}x\color{black}\degree can be observed, with everything above \color{red}x\color{black}\degree being visible throughout the year. The weather is clear and stable on most nights of the year, and there are special laws in place on the island to minimize light pollution and ensuring some of the darkest skies in the world.


\subsection{Telescope, instrument, mode, and setup}
The next big consideration is how to observe the target. First and formost, what telescope should be used? A bigger telescope (usually measured by the size of the primary mirror) can observe the same target and get the same data quality in less time compared to a smaller one. However there will be more competition as bigger telescopes are required to observe fainter targets and the time available more limited than the amount of science people wish to do. This can make it difficult to get the observation time. The ideal telescope size depends on the brightness of the target, big enough to observe it in a reasonable amount of time, but small enough to ensure telescope time is awarded to the project. For instance, the NOT has a 2.56 m primary mirror, while the GTC is a 10 m class telescope. Targets suitable to observe with the NOT may therefore not be consired at the GTC as it may take longer to point the telescope than to observe, wasting a relatively large amount of time during the night.

Secondly, which instrument will give the best results? This depends largely on the type of observations required, and in what mode these observations are (photometry, spectroscopy, polarimetry, to name few). Some telescopes and instruments are designed with a large field of view with a relatively large pixel scale to observe large parts of the sky quickly and systematically in a photometric survey (e.g. ZTF). For more detailed observations however a smaller field of view with less arcseconds per pixel might be preffered. Similarly, there are different ways to do spectroscopy: One can place a slit over the target blocking all other light when getting a spectrum and get spectra of different targets at the same time if the slit is properly oriented. Or one can use fiber spectroscopy, only collecting the light from a specific, small area on the sky. 

Lastly, what setup should be used for the observations? For photometry, which filters should be used? Broadband to get a lot of light at once, or narrowband to focus on a specific wavelength or feature (e.g. H $\alpha$ \color{red}Fix how it is written \color{black})? For spectroscopy, the grating or grism that is used (depending on the instrument) dictates how much different wavelengths are dispersed. A high dispersion will give better wavelength resolution at the cost of being able to observe a smaller part of the spectrum at a time. Similarly, the slit or fiber size dictates the amount of light that reaches the detector. A narrow slit / fiber prevents more stray light from entering but can also block more of the target, which can significantly reduce the signal when the seeing is high. One can additionally request the inclusion of filters, such as a neutral density filter to block out a percentage of the light when observing particularly bright targets, or an order blocking filter to prevent the second order diffraction light of lower wavelengths to be overlayed on top of the first order diffraction light of longer wavelengths.

All of the things above are generally considered together: Different telescopes are in different places and have different observing capabilities, which depend on the instruments that can be attached to the telescope, the modes these instruments allow, the setups that are available, and the difficulty to switch between instruments, modes, and setup. The ALFOSC instrument on the NOT is designed to do both imaging and spectroscopy, and is able to quickly switch between different setups through four \color{red} recheck nr. \color{black} wheels with slits, grisms, and filters. This allows for a great amount of flexibility and the option to observe very different types of targets with very different science goals in the same night.


\subsection{Night plan}
So, you have your list of targets, you got the telescope time, the instrument and setups are installed and calibrated, and you are a few hours away from sunset. The required observation time for each target is calculated to be long enough to obtain the required S/N, but short enough to allow as many other observations as possible. It is time to plan out in detail which object to observe at which time. I have found this to be a very personal process, with each person having their own strategy. I do not wish to critisize any of them, I just aim to outline the general considerations to keep in mind during the process.

Some calibrations, such as bias frames and spectroscopic arc frames, can be taken without the need to point the telescope in a specific direction and are therefore usually taken in the afternoon or morning when no science target can be observed anyway. Some flatfield frames, usually for photometry, are taken during twilight when the sky evenly illuminated but faint enough not to immediately saturate the detector. Different filters need different light levels depending on their properties, which means the flats have to be taken in the correct order as the sky brightness changes during twilight. As the sky darkens, stars become visible and the telescope can be focused. Finally, when the sun is far enough below the horizon and the sky is dark enough, the first science target can be observed, continueing throughout the night untill morning twilight, at which point more sky flats can be taken if required before closing the telescope and ending the observing night.

There are three main ways that time during the night is spent: Target acquisition, exposure, and readout. Exposure is simply the time it takes to do an observation, and as much time as possible should be spent this way as this is where the data is gathered. Readout is the time it takes after an exposure to transfer the data from the instrument to storage. In some cases one might consider to take multiple shorter exposures instead of a single long one (e.g. to resolve time variability or to reduce the impact of cosmic rays on the data), in which case the readout time might become significant and important to keep in mind. Lastly, target acquisition involves everything from moving the telescope to a new target until the start of the first exposure, and the minimum amount of time spent here is largely set by the physical speed at which the telescope, dome, and instrument can get into the correct position. In an efficient night plan this time can be decreased by observing targets in the order that minimizes the telescope movement between them, while keeping other constraint such as altitude, sky brightness, and moon distance in mind.

Of course, weather conditions vary and changing observing conditions can significantly impact the night plan. High winds or partial cloud coverage can restrict the direction in which can be observed or even prevent observations altogether throughout the night, while high seeing can prevent observations to reached the desired quality. Flexibility during the night is therefore required, and it is wise to both have a priority list and backup targets in case the night plan has to be updated on the fly. After all, an idling telescope in (half-)decent observing conditions is a waste of resources.

\section{Types of obeservations}
As mentioned above, there are many different types of observations that can be taken, and every instrument will have a unique set of observation types available. Discussing these types in detail is far out of the scope of this text, and better resources for this already exist. \color{red} reference some books or so about observing techniques or something, edit this part to make it flow better \color{black} This will only be a general overview of the two most commonly used and well-known types of observations: photometry and spectroscopy. The different types of calibration images are handled here as well.

\subsection{Photometry}
Photometry, in its simplest essence, is nothing else than looking at a part of the sky and taking a photo. The difference between a regular camera and a professional instrument is how precise the light levels are measured, allowing to observe the faintest sources. Noise is reduced through the specially designed instrument and by using calibration images. Carefully constructed filters with known a level of transmission as function of wavelength are placed in the light path to ensure only a specific, known part of the spectrum hits the detector. This way the received flux from a source can be carefully measured. By observing stars whose brightness is known under similar conditions the flux can be calibrated to get measure of the targets brightness.

\subsection{Spectrocopy}
Spectroscopy takes the concept of image taking one step further. Instead of a filter to select a wavelength range to observe, now a slit restricts the observable region of the sky to a narrow band along one axis of the detector (e.g. horizontal). After the slit the light hits a grating or grism (a grating and prism combined) which diffracts the light based on wavelength across the second axis of the detector (vertical). The rule density on the grating / grism dictates the wavelength spread of the light, the more rules per unit distance, the bigger the diffraction, and the higher the spectral resolution of the resulting image. The tradeoff is that a smaller part of the spectrum can be observed at a time, and there is less light being received per pixel which reduces the SNR unless the exposure time is increased to account for this. the observed object has become a line in the spectral direction, which is called a trace.

In theory the slit can be freely rotated on the sky by simply rotating the instrument. This is useful in case there are two targets close together, so they can be observed at the same time. If no specific orientation is required, it is usually a good idea to rotate the slit such that is is vertical with respect to the horizon. \color{red} figure out how to best put parallactic angle name in here \color{black} Starlight that enters the earths atmosphere at an angle is refracted, resulting in a vertical split in colour. The higher the airmass, the bigger this split is, and it can result in a part of the light being blocked by the slit if this is not taken into account. \color{red} cite that paper \color{black}

As with photometry, to get a proper flux calibration a star with a known spectrum has to be observed as well. In photometry many stars in a given field can have well-observed and documented magnitudes if a common filter is used, such as the SDSS $gri$ filters. For spectroscopy however it is very unusual to have a star that can be used for calibrations in the same observation as the science target. Instead, a spectrophotometric standard star (a star with a well-known spectrum, often having few elemental lines) has to be observed in the same setup and as close to the same conditions as possible.

\subsection{Calibrations}
To get the cleanest signals it is important to reduce noise as much as possible. Several sources of noise originating from the instrument can be removed using different types of calibration images. Generally speaking there are three types of calibration images for photometry, and one extra for spectroscopy. The response function of a detector can be written as \color{red} Probably good to go over with someone at some point to be sure there's no mistake here, also need references of course. make sure CCD is properly introduced somewhere above. Need to talk about gain and read noise as well somewhere, read up on that and add. \color{black}

\begin{equation}
	R_{ij}(F, t) = B_{ij} + D_{ij} \times t + A_{ij} \times F,
	\label{CCD_response}
\end{equation}

where $R_{ij}$ is the CCD response of pixel $i,j$ as a function of the integrated flux $F$ during the exposure which lasted a time $t$. The goal is to measure the flux $f = F/t$, which requires knowing $A_{ij}$, $B_{ij}$, and $D_{ij}$. Each type of calibration image is used to measure one of these values. Note that it is assumed that there are no cross or higher order terms in Eq.~\ref{CCD_response}, in other words, the CCD is in its linear regime. When a pixel receives too much light and gets close to saturation it is no longer in its linear regime, and more terms appear in Eq.~\ref{CCD_response} making it much more difficult or even impossible to measure the observed flux.

\subsubsection*{Bias}
$B_{ij}$ is the so-called bias, the measured pixel response that is in every observation regardless of the exposure time and amount of light hitting the pixel. The easiest way to measure this value is to take bias frames: observe with an exposure time of 0 with no light hitting the CCD. $F$ and $t$ are 0 which reduces Eq.~\ref{CCD_response} to $R_{ij}(0, 0) = B_{ij}$

\subsubsection*{Dark}
Next is $D_{ij}$, the noise that increases with longer exposure times. This is thermal noise, and as its name suggests it is more impactful the higher the temperature $T$ is in the CCD: $D_{ij} = D_{ij}(T)$. Due to this there are actually two ways to remove this source of noise. One can take Dark frames by exposing for some amount of time but making sure no light hits the detector, $R_{ij}(0, t) = B_{ij} + D_{ij} \times t$, and extract $D_{ij}$. This takes however a lot of time, and it is often more efficient to cool the CCD down to very low temperatures (e.g. with liquid nitrogen) to ensure $D_{ij}$ can be neglected.

\subsubsection*{Flatfield}
Lastly, to measure $A_{ij}$ flatfield images are needed. During a science exposure the amount of light hitting a pixel is dependent of the brightness of the source at that location, $F = F_{ij}$. But as not every pixel may have the same response the measured values need to be normalized before different pixels can be compared. This is done by observing an evenly lit background where $F_{ij}$ is the same for every pixel. This can be done by observing the inside of the dome (dome flats), although the twilight sky (sky flats) are usually preferred as they are more evenly lit across the CCD.

\subsubsection*{Arc}
For spectroscopy another type of calibration is needed: Wavelength calibration. This is done with arc frames: observing the known emission lines of several elements using lamps in the instrument (e.g. He, Ne, ThAr lamps) using the same setup as the science observation. The resulting pattern of emission lines is known and can be used to map the pixel position in the spectral direction to a wavelength, called the wavelength solution.


\section{Analysis}
After all observations have been taken it is time to analyze them. The first step in this is to reduce the raw data into the required format to work with. After that, additional analysis technique can manipulate the reduced images directly or the data that has been extracted from them. I will briefly discuss difference imaging and forced photometry, as these are important for this thesis. \color{red} More techniques that need discussion? Might want to make this its own chapter separate from observing.\color{black}

\subsection{Reduction}
Using the calibration images from above, the raw science images can be reduced to something a flux level can be measured from. Usually several calibration images of a type are taken to average out and remove outliers due to e.g. cosmic rays, which is usually called the master image. First the master-bias is created and subtracted from every other image. Then the same can be done with the master-dark if needed. Lastly every science image is divided through the normalized master-flat to equalize the pixel responses. Once the images are reduced it is often good practice to run a cosmic-ray removal algorith to remove this source of noise as much as possible \color{red} Check name and add references \color{black}.

If multiple images are taken of the same field or object. one can stack them at this stage do reduce noise and increase the SNR of the observed objects. Sometimes the observations were taken with slightly different telescope pointings (dithering) to avoid the same objects being on the same pixels every time. This is good to keep in mind to make sure that the images are stacked correctly.

With photometry the observed brightness can be measured for each star in the image to get a list of instrument magnitudes. The relative differences between the magnitudes of objects are correct, but there is still an absolute offset across all objects. This is corrected by comparing with known values from the literature.

For spectroscopy, after the calibrations above the wavelength solution is added in the spectral direction of the image. The trace is extracted and converted into a 1D spectrum, but is not yet calibrated for the detector efficiency at different wavelengths. This is where the standard star comes in. By comparing the observed spectrum to the known one, the detector wavelength response function can be recovered. Applying this to the science observation provides the flux calibration, ensuring that the observed flux at different wavelengths can be compared.

\subsection{Difference imaging}
A popular method for discovering and isolating transients is to look at what has changed on the sky. If there is a map of how a part of the sky looked some time ago, the constant sources such as most stars (except variable ones) and galaxies can be removed. To computers, images are nothing more than big matrices. This means that, if properly aligned, subtracting the reference from the science images is an easy operation that leaves only those sources whose brigthness has changed between the two observations in the so-called difference image. This also ensures that only the transient light is measured in cases such as a SN superimposed on top of its host galaxy.

\subsection{Forced photometry}
The two most common methods to measure flux from an image are PSF photometry and aperture photometry. PSF stands for point spread function, and with this method a function is fitted to model the source. This function describes how an infinitely small point of light is spread over the detector, and through its spatial size and peak value the flux of the light source can be measured. Aperture photometry sums up the signal in a given radius around the source center and subtracts the contribution from the background in the same region.

\color{red} See \url{https://coolwiki.ipac.caltech.edu/index.php/Aperture_Photometry_Overview} for references \color{black}

Large surveys such as ZTF observe the night sky to find new transients and monitor known ones. Difference imaging is used to reveal active transients, as these are the main sources that should be left in the difference image. Through PSF photometry the location and strength of each source in the image is determined, which are then compared to the locations of known sources to separate new from known ones. Each location has however been observed for the entire duration of the survey, which means that it is also possible to measure the flux of a known transient before and after it was visible in the images, creating a light curve for the full duration of the survey.

This is called forced photometry, because the PSF function is forced to center on a specific location instead of finding the best-fitting position for the centroid. When there is nothing but noise at that location the measured flux will be 0 within the error. When there is a source at the target location it will be measured, but if the source is not at the center of the PSF the fit will have trouble converging, resulting in a large uncertainty.

\subsection{The SuperNova Animation Program}
The light curve that is the result of the forced photometry above can contain bad data points. Many of these will be flagged for having a bad PSF fit, bad weather conditions, etc. But even after filtering these out it can be helpful to check the difference images themselves for unexpected behavouir that might be captured in the light curve. For this I created the SuperNova Animation Program (\textsc{snap})

\textsc{snap} collects image cutouts of the specified location during the  specified time period(s) and in the specified filter(s). It then matches these with the individual data points in the light curve and puts the images in an animation in chronological order with the reference images at the start. This enables easy identification of bad points due to image defects, off-center sources, residual from imperfectly subtracted sources, cosmic rays etc.

\subsection{Simulations}
Some experiments are difficult or even impossible to do multiple times, one cannot rerun a survey to observe the same transient events. So to understand the biases in the data as well as the effective size of the survey, it needs to be simulated.

\end{document}